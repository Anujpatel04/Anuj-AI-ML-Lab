# Fine-Tuning Configuration for Code LLM
# Optimized for MacBook M4 (Apple Silicon)

model:
  # DeepSeek-Coder-1.3B: Small, code-focused, perfect for M4
  # Alternative: "Qwen/Qwen-Coder-1.5B-7B" or "bigcode/starcoder2-3b"
  name: "deepseek-ai/DeepSeek-Coder-1.3B-Instruct"
  
  # Quantization settings for memory efficiency
  # IMPORTANT: MPS (GPU) training can hang - CPU with quantization is more reliable
  # Set to true for CPU training (recommended for stability)
  # Set to false to try MPS GPU (may hang - if so, switch back to true)
  load_in_4bit: true  # true = CPU (stable), false = MPS GPU (may hang)

# LoRA Configuration
lora:
  r: 16  # Rank (lower = less parameters, higher = more capacity)
  alpha: 32  # LoRA alpha (typically 2x rank)
  dropout: 0.1  # Dropout rate
  target_modules:  # Modules to apply LoRA to
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Dataset Configuration
data:
  dataset_path: "dataset.jsonl"  # Path to prepared dataset

# Training Configuration
training:
  output_dir: "outputs"  # Where to save checkpoints and final model
  num_epochs: 3  # Number of training epochs
  batch_size: 1  # Batch size per device (keep small for M4)
  gradient_accumulation_steps: 8  # Effective batch size = batch_size * gradient_accumulation_steps
  learning_rate: 2.0e-4  # Learning rate
  max_length: 2048  # Maximum sequence length
  val_split: 0.1  # Validation split ratio
  
  # Logging and saving
  logging_steps: 10  # Log every N steps
  save_steps: 100  # Save checkpoint every N steps
  eval_steps: 100  # Evaluate every N steps
  save_total_limit: 3  # Keep only last N checkpoints
  warmup_steps: 100  # Warmup steps for learning rate

# Inference Configuration (for reference)
inference:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  do_sample: true

